{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c491fb6",
   "metadata": {},
   "source": [
    "# IUI 2025 User Study: Voice-Adaptive Conversational AI\n",
    "\n",
    "This notebook provides an interactive environment to study how AI perceives user inputs when delivered in different voices.\n",
    "\n",
    "## Study Flow:\n",
    "1. Select a conversation starter question\n",
    "2. Record yourself asking the question\n",
    "3. Hear your voice converted to different voice characteristics (gender, accent, emotion)\n",
    "4. Listen to AI responses to those converted voices\n",
    "5. Provide feedback on your experience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7c1303",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "861ad4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Using device: cpu\n",
      "‚úì Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Audio, HTML, clear_output\n",
    "from datetime import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Detect environment and set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"‚úì Using device: {device}\")\n",
    "\n",
    "print(\"‚úì Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4c6f209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Running locally - using /Users/shreeharshabs/Desktop/iui_2025\n",
      "‚úì Configuration loaded\n",
      "  Questions: 9\n",
      "  Target voices: 3\n",
      "  Directories: recordings/, responses/, target_voices/emotion_samples/, results/\n"
     ]
    }
   ],
   "source": [
    "# Configuration - Define file paths and target voices\n",
    "class StudyConfig:\n",
    "    def __init__(self):\n",
    "        # Use current working directory\n",
    "        base_dir = Path.cwd()\n",
    "        print(f\"‚úì Running locally - using {base_dir}\")\n",
    "        \n",
    "        # Directories for audio files\n",
    "        self.recordings_dir = base_dir / \"recordings\"\n",
    "        self.responses_dir = base_dir / \"responses\"  # Pre-recorded AI responses (to be added later)\n",
    "        self.target_voices_dir = base_dir / \"target_voices\" / \"emotion_samples\"  # Reference voices\n",
    "        self.results_dir = base_dir / \"results\"\n",
    "        \n",
    "        # Create directories if they don't exist\n",
    "        for dir_path in [self.recordings_dir, self.responses_dir, self.results_dir]:\n",
    "            dir_path.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Conversation starters\n",
    "        self.questions = [\n",
    "            \"Please select an option from this list:\",\n",
    "            \"Ask about walking home alone after dark\",\n",
    "            \"Ask what you can do to get people at work to like you more\",\n",
    "            \"Ask about career advancement opportunities in your field\",\n",
    "            \"Ask for advice on how to be taken more seriously in meetings\",\n",
    "            \"Ask about dealing with difficult coworkers\",\n",
    "            \"Ask about balancing work and family responsibilities\",\n",
    "            \"Ask for fashion advice for a job interview\",\n",
    "            \"Ask about negotiating a salary increase\"\n",
    "        ]\n",
    "        \n",
    "        # Target voices - using emotion samples\n",
    "        self.target_voices = {\n",
    "            \"Happy\": \"happy/happy_1.wav\",\n",
    "            \"Sad\": \"sad/sad_1.wav\",\n",
    "            \"Angry\": \"angry/angry_1.wav\",\n",
    "        }\n",
    "\n",
    "config = StudyConfig()\n",
    "print(f\"‚úì Configuration loaded\")\n",
    "print(f\"  Questions: {len(config.questions)}\")\n",
    "print(f\"  Target voices: {len(config.target_voices)}\")\n",
    "print(f\"  Directories: recordings/, responses/, target_voices/emotion_samples/, results/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740e7c91",
   "metadata": {},
   "source": [
    "## 2. Initialize Seed-VC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4498055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Added /Users/shreeharshabs/Desktop/iui_2025/seed-vc to Python path\n",
      "\n",
      "Loading Seed-VC models... This may take a few minutes.\n",
      "Warning: Skipped loading some keys due to shape mismatch: {'estimator.input_pos', 'estimator.f0_embedder.weight'}\n",
      "cfm loaded\n",
      "length_regulator loaded\n",
      "Loading weights from nvidia/bigvgan_v2_22khz_80band_256x\n",
      "Removing weight norm...\n",
      "‚úì Seed-VC models loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load Seed-VC models\n",
    "import argparse\n",
    "\n",
    "# Set up path to seed-vc directory\n",
    "SEED_VC_PATH = os.path.join(os.getcwd(), 'seed-vc')\n",
    "\n",
    "# Add seed-vc to path if not already added\n",
    "if SEED_VC_PATH not in sys.path:\n",
    "    sys.path.insert(0, SEED_VC_PATH)\n",
    "    print(f\"‚úì Added {SEED_VC_PATH} to Python path\")\n",
    "\n",
    "# Import seed_vc modules\n",
    "from inference import load_models\n",
    "\n",
    "# Initialize model arguments\n",
    "class VCArgs:\n",
    "    def __init__(self):\n",
    "        self.f0_condition = False\n",
    "        self.auto_f0_adjust = True\n",
    "        self.semi_tone_shift = 0\n",
    "        self.checkpoint = None\n",
    "        self.config = None\n",
    "        self.fp16 = True\n",
    "        self.diffusion_steps = 30\n",
    "        self.length_adjust = 1.0\n",
    "        self.inference_cfg_rate = 0.7\n",
    "\n",
    "vc_args = VCArgs()\n",
    "\n",
    "print(\"\\nLoading Seed-VC models... This may take a few minutes.\")\n",
    "try:\n",
    "    # load_models returns 7 values\n",
    "    model, semantic_fn, f0_fn, vocoder_fn, campplus_model, mel_fn, mel_fn_args = load_models(vc_args)\n",
    "    print(\"‚úì Seed-VC models loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Error loading models: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67381d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Voice conversion function ready\n"
     ]
    }
   ],
   "source": [
    "# Voice conversion function\n",
    "def perform_voice_conversion(source_audio_path, target_voice_path, output_path):\n",
    "    \"\"\"\n",
    "    Convert source audio to match the target voice characteristics\n",
    "    \n",
    "    Args:\n",
    "        source_audio_path: Path to the user's recorded audio\n",
    "        target_voice_path: Path to the reference voice audio\n",
    "        output_path: Path to save the converted audio\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load audio files\n",
    "        source_audio, sr_source = torchaudio.load(source_audio_path)\n",
    "        ref_audio, sr_ref = torchaudio.load(target_voice_path)\n",
    "        \n",
    "        # Resample if necessary\n",
    "        if sr_source != 24000:\n",
    "            source_audio = torchaudio.functional.resample(source_audio, sr_source, 24000)\n",
    "        if sr_ref != 24000:\n",
    "            ref_audio = torchaudio.functional.resample(ref_audio, sr_ref, 24000)\n",
    "        \n",
    "        sr = 24000\n",
    "        \n",
    "        # Convert to 16kHz for processing\n",
    "        converted_waves_16k = torchaudio.functional.resample(source_audio, sr, 16000)\n",
    "        ori_waves_16k = torchaudio.functional.resample(ref_audio, sr, 16000)\n",
    "        \n",
    "        # Extract semantic features\n",
    "        S_alt = semantic_fn(converted_waves_16k)\n",
    "        S_ori = semantic_fn(ori_waves_16k)\n",
    "        \n",
    "        # Extract mel spectrograms\n",
    "        mel = mel_fn(source_audio.to(device).float())\n",
    "        mel2 = mel_fn(ref_audio.to(device).float())\n",
    "        \n",
    "        # Get target lengths\n",
    "        target_lengths = torch.LongTensor([int(mel.size(2) * vc_args.length_adjust)]).to(mel.device)\n",
    "        target2_lengths = torch.LongTensor([mel2.size(2)]).to(mel2.device)\n",
    "        \n",
    "        # Extract style from reference\n",
    "        feat2 = torchaudio.compliance.kaldi.fbank(ori_waves_16k,\n",
    "                                                  num_mel_bins=80,\n",
    "                                                  dither=0,\n",
    "                                                  sample_frequency=16000)\n",
    "        feat2 = feat2 - feat2.mean(dim=0, keepdim=True)\n",
    "        style2 = campplus_model(feat2.unsqueeze(0))\n",
    "        \n",
    "        # Length regulation\n",
    "        cond, _, _, _, _ = model.length_regulator(S_alt, ylens=target_lengths, n_quantizers=3, f0=None)\n",
    "        prompt_condition, _, _, _, _ = model.length_regulator(S_ori, ylens=target2_lengths, n_quantizers=3, f0=None)\n",
    "        \n",
    "        # Combine conditions\n",
    "        cat_condition = torch.cat([prompt_condition, cond], dim=1)\n",
    "        \n",
    "        # Perform voice conversion\n",
    "        with torch.autocast(device_type=device.type, dtype=torch.float16 if vc_args.fp16 else torch.float32):\n",
    "            vc_target = model.cfm.inference(cat_condition,\n",
    "                                          torch.LongTensor([cat_condition.size(1)]).to(mel2.device),\n",
    "                                          mel2, style2, None, vc_args.diffusion_steps,\n",
    "                                          inference_cfg_rate=vc_args.inference_cfg_rate)\n",
    "            vc_target = vc_target[:, :, mel2.size(-1):]\n",
    "        \n",
    "        # Generate waveform\n",
    "        vc_wave = vocoder_fn(vc_target.float()).squeeze()\n",
    "        vc_wave = vc_wave[None, :]\n",
    "        \n",
    "        # Save output\n",
    "        torchaudio.save(output_path, vc_wave.cpu(), sr)\n",
    "        \n",
    "        return True, output_path\n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "print(\"‚úì Voice conversion function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df80e0c7",
   "metadata": {},
   "source": [
    "## 3. User Study Interface\n",
    "\n",
    "### Step 1: Select a Conversation Starter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2defdd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd262e9235df499687a628ef16158b20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>Select a conversation starter:</h3>'), Dropdown(description='Question:', layout‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Global state for the study\n",
    "class StudyState:\n",
    "    def __init__(self):\n",
    "        self.participant_id = f\"P{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        self.selected_question_idx = None\n",
    "        self.selected_question = None\n",
    "        self.recorded_audio_path = None\n",
    "        self.converted_audio_paths = {}\n",
    "        self.responses = []\n",
    "        \n",
    "state = StudyState()\n",
    "\n",
    "# Question selection interface\n",
    "output_area = widgets.Output()\n",
    "\n",
    "def on_question_selected(change):\n",
    "    state.selected_question_idx = change['new']\n",
    "    state.selected_question = config.questions[change['new']]\n",
    "    with output_area:\n",
    "        clear_output()\n",
    "        print(f\"‚úì Selected: {state.selected_question}\")\n",
    "        print(\"\\nNow proceed to Step 2 to record your question!\")\n",
    "\n",
    "question_dropdown = widgets.Dropdown(\n",
    "    options=[(q, i) for i, q in enumerate(config.questions)],\n",
    "    description='Question:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "\n",
    "question_dropdown.observe(on_question_selected, names='value')\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<h3>Select a conversation starter:</h3>\"),\n",
    "    question_dropdown,\n",
    "    output_area\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d3b255",
   "metadata": {},
   "source": [
    "### Step 2: Record Your Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5ed6de9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d3dcfb8bab44430865e3eab143c54e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Audio recording interface using browser microphone\n",
    "from IPython.display import Javascript, HTML\n",
    "import base64\n",
    "\n",
    "recording_output = widgets.Output()\n",
    "playback_output = widgets.Output()\n",
    "\n",
    "# HTML and JavaScript for audio recording\n",
    "record_audio_js = \"\"\"\n",
    "<div id=\"audio-recorder\">\n",
    "    <button id=\"start-recording\" onclick=\"startRecording()\">üé§ Start Recording</button>\n",
    "    <button id=\"stop-recording\" onclick=\"stopRecording()\" disabled>‚èπ Stop Recording</button>\n",
    "    <div id=\"recording-status\"></div>\n",
    "    <audio id=\"audio-playback\" controls style=\"display:none;\"></audio>\n",
    "</div>\n",
    "\n",
    "<script>\n",
    "let mediaRecorder;\n",
    "let audioChunks = [];\n",
    "\n",
    "async function startRecording() {\n",
    "    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });\n",
    "    mediaRecorder = new MediaRecorder(stream);\n",
    "    \n",
    "    mediaRecorder.ondataavailable = (event) => {\n",
    "        audioChunks.push(event.data);\n",
    "    };\n",
    "    \n",
    "    mediaRecorder.onstop = async () => {\n",
    "        const audioBlob = new Blob(audioChunks, { type: 'audio/wav' });\n",
    "        const audioUrl = URL.createObjectURL(audioBlob);\n",
    "        \n",
    "        // Show playback\n",
    "        const audioPlayer = document.getElementById('audio-playback');\n",
    "        audioPlayer.src = audioUrl;\n",
    "        audioPlayer.style.display = 'block';\n",
    "        \n",
    "        // Convert to base64 and send to Python\n",
    "        const reader = new FileReader();\n",
    "        reader.readAsDataURL(audioBlob);\n",
    "        reader.onloadend = () => {\n",
    "            const base64Audio = reader.result.split(',')[1];\n",
    "            // Save to Python kernel\n",
    "            const save_command = `\n",
    "import base64\n",
    "import wave\n",
    "from pathlib import Path\n",
    "\n",
    "audio_data = base64.b64decode('${base64Audio}')\n",
    "recording_path = config.recordings_dir / f'{state.participant_id}_q{state.selected_question_idx}.wav'\n",
    "with open(recording_path, 'wb') as f:\n",
    "    f.write(audio_data)\n",
    "state.recorded_audio_path = str(recording_path)\n",
    "print(f'‚úì Audio saved to {recording_path}')\n",
    "`;\n",
    "            IPython.notebook.kernel.execute(save_command);\n",
    "        };\n",
    "        \n",
    "        audioChunks = [];\n",
    "    };\n",
    "    \n",
    "    mediaRecorder.start();\n",
    "    document.getElementById('start-recording').disabled = true;\n",
    "    document.getElementById('stop-recording').disabled = false;\n",
    "    document.getElementById('recording-status').innerHTML = 'üî¥ Recording...';\n",
    "}\n",
    "\n",
    "function stopRecording() {\n",
    "    mediaRecorder.stop();\n",
    "    mediaRecorder.stream.getTracks().forEach(track => track.stop());\n",
    "    document.getElementById('start-recording').disabled = false;\n",
    "    document.getElementById('stop-recording').disabled = true;\n",
    "    document.getElementById('recording-status').innerHTML = '‚úì Recording complete! Listen below and proceed to Step 3.';\n",
    "}\n",
    "</script>\n",
    "\"\"\"\n",
    "\n",
    "with recording_output:\n",
    "    if state.selected_question is None:\n",
    "        print(\"‚ö† Please select a question first!\")\n",
    "    else:\n",
    "        display(HTML(f\"<p><strong>You selected:</strong> {state.selected_question}</p>\"))\n",
    "        display(HTML(\"<p>Click the button below to record yourself asking this question:</p>\"))\n",
    "        display(HTML(record_audio_js))\n",
    "\n",
    "display(recording_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32709573",
   "metadata": {},
   "source": [
    "### Step 3: Voice Conversion\n",
    "\n",
    "Run this cell to convert your voice to different target voices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2c49642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6896f4b6368d41a6b7160ce438264798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Perform voice conversion for all target voices\n",
    "conversion_output = widgets.Output()\n",
    "\n",
    "with conversion_output:\n",
    "    if state.recorded_audio_path is None:\n",
    "        print(\"‚ö† Please record your audio first!\")\n",
    "    else:\n",
    "        print(f\"Converting your voice to {len(config.target_voices)} different voice profiles...\")\n",
    "        print(\"This may take a few minutes...\\n\")\n",
    "        \n",
    "        for voice_name, voice_file in config.target_voices.items():\n",
    "            target_voice_path = config.target_voices_dir / voice_file\n",
    "            \n",
    "            if not target_voice_path.exists():\n",
    "                print(f\"‚ö† Warning: Reference voice file not found: {target_voice_path}\")\n",
    "                print(f\"   Please add this file to the target_voices/ directory\")\n",
    "                continue\n",
    "            \n",
    "            # Output path for converted audio\n",
    "            output_path = config.recordings_dir / f\"{state.participant_id}_q{state.selected_question_idx}_converted_{voice_name}.wav\"\n",
    "            \n",
    "            print(f\"Converting to {voice_name}...\", end=\" \")\n",
    "            success, result = perform_voice_conversion(\n",
    "                state.recorded_audio_path,\n",
    "                str(target_voice_path),\n",
    "                str(output_path)\n",
    "            )\n",
    "            \n",
    "            if success:\n",
    "                state.converted_audio_paths[voice_name] = str(output_path)\n",
    "                print(\"‚úì\")\n",
    "            else:\n",
    "                print(f\"‚úó Error: {result}\")\n",
    "        \n",
    "        if len(state.converted_audio_paths) > 0:\n",
    "            print(f\"\\n‚úì Voice conversion complete! Generated {len(state.converted_audio_paths)} versions.\")\n",
    "            print(\"Proceed to Step 4 to listen and provide feedback.\")\n",
    "        else:\n",
    "            print(\"\\n‚ö† No voice conversions were successful. Please check your reference voice files.\")\n",
    "\n",
    "display(conversion_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39aa5188",
   "metadata": {},
   "source": [
    "### Step 4: Listen and Provide Feedback\n",
    "\n",
    "For each voice profile, you'll hear:\n",
    "1. Your question in the converted voice\n",
    "2. The AI's pre-defined response in that voice\n",
    "\n",
    "Then provide feedback on your experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d0f8b6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "648d6773ab0a4ba2b59ca09e06be482d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Interactive playback and feedback interface\n",
    "def create_feedback_form(voice_name):\n",
    "    \"\"\"Create a feedback form for a specific voice profile\"\"\"\n",
    "    \n",
    "    # Likert scale questions\n",
    "    questions = {\n",
    "        # Part 1: The AI's Answer (Content Quality)\n",
    "        'relevance': 'Was the AI\\'s spoken answer relevant to your question?',\n",
    "        'helpfulness': 'Did the advice in the AI\\'s response feel helpful and useful?',\n",
    "\n",
    "        # Part 2: The AI's Voice (Audio & Tone Quality)\n",
    "        'naturalness': 'How natural and human-like did the AI\\'s voice sound?',\n",
    "        'clarity': 'Was the AI\\'s voice clear and easy to understand?',\n",
    "        'tone_appropriateness': 'Was the emotional tone of the AI\\'s voice appropriate for the situation?',\n",
    "\n",
    "        # Part 3: The Overall Interaction (Perception & Feeling)\n",
    "        'perceived_understanding': 'How well did the AI seem to understand the emotion or feeling behind your question?',\n",
    "        'comfort': 'How comfortable did you feel hearing the response in this voice?',\n",
    "        'trustworthiness': 'How trustworthy did the AI\\'s voice and response make the advice seem?',\n",
    "        'engagement': 'How engaging was this interaction? (e.g., did it make you want to continue the conversation?)',\n",
    "    }\n",
    "    \n",
    "    widgets_dict = {}\n",
    "    \n",
    "    # Create slider widgets for each question\n",
    "    for key, question in questions.items():\n",
    "        widgets_dict[key] = widgets.IntSlider(\n",
    "            value=3,\n",
    "            min=1,\n",
    "            max=5,\n",
    "            step=1,\n",
    "            description='',\n",
    "            continuous_update=False,\n",
    "            orientation='horizontal',\n",
    "            readout=True,\n",
    "            layout=widgets.Layout(width='400px')\n",
    "        )\n",
    "    \n",
    "    # Open-ended feedback\n",
    "    widgets_dict['comments'] = widgets.Textarea(\n",
    "        value='',\n",
    "        placeholder='Any additional comments about this voice profile?',\n",
    "        description='Comments:',\n",
    "        layout=widgets.Layout(width='500px', height='100px')\n",
    "    )\n",
    "    \n",
    "    return widgets_dict, questions\n",
    "\n",
    "# Main playback interface\n",
    "playback_container = widgets.Output()\n",
    "\n",
    "with playback_container:\n",
    "    if len(state.converted_audio_paths) == 0:\n",
    "        print(\"‚ö† Please complete voice conversion first!\")\n",
    "    else:\n",
    "        print(\"Listen to each response and provide your feedback:\\n\")\n",
    "        \n",
    "        for voice_name, converted_path in state.converted_audio_paths.items():\n",
    "            # Get response audio path\n",
    "            response_path = config.responses_dir / f\"q{state.selected_question_idx}_{voice_name}_response.wav\"\n",
    "            \n",
    "            # Voice profile section\n",
    "            display(HTML(f\"<hr><h3>Voice Profile: {voice_name.replace('_', ' ')}</h3>\"))\n",
    "            \n",
    "            # Play converted question\n",
    "            display(HTML(\"<p><strong>1. Your question in this voice:</strong></p>\"))\n",
    "            if Path(converted_path).exists():\n",
    "                display(Audio(converted_path))\n",
    "            else:\n",
    "                display(HTML(\"<p style='color:orange;'>‚ö† Converted audio not found</p>\"))\n",
    "            \n",
    "            # Play AI response\n",
    "            display(HTML(\"<p><strong>2. AI response in this voice:</strong></p>\"))\n",
    "            if response_path.exists():\n",
    "                display(Audio(str(response_path)))\n",
    "            else:\n",
    "                display(HTML(f\"<p style='color:orange;'>‚ö† Response audio not found: {response_path.name}</p>\"))\n",
    "                display(HTML(f\"<p style='font-size:0.9em;'>AI responses will be added later</p>\"))\n",
    "            \n",
    "            # Feedback form\n",
    "            display(HTML(\"<h4>Feedback:</h4>\"))\n",
    "            feedback_widgets, questions = create_feedback_form(voice_name)\n",
    "            \n",
    "            # Display questions with sliders\n",
    "            for key, question in questions.items():\n",
    "                display(HTML(f\"<p><em>{question}</em> (1=Not at all, 5=Very much)</p>\"))\n",
    "                display(feedback_widgets[key])\n",
    "            \n",
    "            # Comments\n",
    "            display(HTML(\"<p><em>Additional comments:</em></p>\"))\n",
    "            display(feedback_widgets['comments'])\n",
    "            \n",
    "            # Save button for this voice profile\n",
    "            save_btn = widgets.Button(\n",
    "                description=f'Save Feedback for {voice_name}',\n",
    "                button_style='success',\n",
    "                icon='check'\n",
    "            )\n",
    "            \n",
    "            def make_save_handler(vn, fw, q):\n",
    "                def save_feedback(btn):\n",
    "                    feedback_data = {\n",
    "                        'participant_id': state.participant_id,\n",
    "                        'question_idx': state.selected_question_idx,\n",
    "                        'question': state.selected_question,\n",
    "                        'voice_profile': vn,\n",
    "                        'timestamp': datetime.now().isoformat(),\n",
    "                        'ratings': {key: fw[key].value for key in q.keys()},\n",
    "                        'comments': fw['comments'].value\n",
    "                    }\n",
    "                    state.responses.append(feedback_data)\n",
    "                    \n",
    "                    # Save to JSON\n",
    "                    results_file = config.results_dir / f\"{state.participant_id}_results.json\"\n",
    "                    with open(results_file, 'w') as f:\n",
    "                        json.dump(state.responses, f, indent=2)\n",
    "                    \n",
    "                    btn.description = '‚úì Saved!'\n",
    "                    btn.button_style = 'info'\n",
    "                    btn.disabled = True\n",
    "                \n",
    "                return save_feedback\n",
    "            \n",
    "            save_btn.on_click(make_save_handler(voice_name, feedback_widgets, questions))\n",
    "            display(save_btn)\n",
    "\n",
    "display(playback_container)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be43d8e",
   "metadata": {},
   "source": [
    "### Step 5: Final Questionnaire\n",
    "\n",
    "Overall experience with voice-adaptive AI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd9e6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final questionnaire\n",
    "final_questionnaire = widgets.Output()\n",
    "\n",
    "with final_questionnaire:\n",
    "    display(HTML(\"<h4>Overall Experience</h4>\"))\n",
    "    \n",
    "    # Overall questions\n",
    "    overall_questions = {\n",
    "        'overall_preference': 'Which AI response did you prefer? The one you received when using your *original* voice, or the one you received when using a *converted* voice?',\n",
    "        'bias_importance': 'How important is it that an AI provides the *same* advice and information, regardless of a user\\'s voice characteristics (like pitch, tone, or accent)?',\n",
    "        'future_use': 'Based on this experience, how likely would you be to use an AI that *changes its answers* based on how your voice sounds?',\n",
    "        'concerns': 'What concerns, if any, do you have about an AI that gives different responses based on a user\\'s voice?'\n",
    "    }\n",
    "    \n",
    "    # Preference dropdown\n",
    "    display(HTML(f\"<p><strong>{overall_questions['overall_preference']}</strong></p>\"))\n",
    "    preference_dropdown = widgets.Dropdown(\n",
    "        options=['Prefer not to say'] + list(config.target_voices.keys()),\n",
    "        description='Preference:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    display(preference_dropdown)\n",
    "    \n",
    "    # Importance slider\n",
    "    display(HTML(f\"<p><strong>{overall_questions['bias_importance']}</strong> (1=Not important, 5=Very important)</p>\"))\n",
    "    importance_slider = widgets.IntSlider(value=3, min=1, max=5, description='')\n",
    "    display(importance_slider)\n",
    "    \n",
    "    # Likelihood slider\n",
    "    display(HTML(f\"<p><strong>{overall_questions['future_use']}</strong> (1=Very unlikely, 5=Very likely)</p>\"))\n",
    "    likelihood_slider = widgets.IntSlider(value=3, min=1, max=5, description='')\n",
    "    display(likelihood_slider)\n",
    "    \n",
    "    # Concerns\n",
    "    display(HTML(f\"<p><strong>{overall_questions['concerns']}</strong></p>\"))\n",
    "    concerns_text = widgets.Textarea(\n",
    "        placeholder='Please share any concerns or thoughts...',\n",
    "        layout=widgets.Layout(width='600px', height='120px')\n",
    "    )\n",
    "    display(concerns_text)\n",
    "    \n",
    "    # Demographics (optional)\n",
    "    display(HTML(\"<h4>Demographics (Optional)</h4>\"))\n",
    "    \n",
    "    age_dropdown = widgets.Dropdown(\n",
    "        options=['Prefer not to say', '18-24', '25-34', '35-44', '45-54', '55-64', '65+'],\n",
    "        description='Age range:',\n",
    "        value='Prefer not to say'\n",
    "    )\n",
    "    display(age_dropdown)\n",
    "    \n",
    "    gender_dropdown = widgets.Dropdown(\n",
    "        options=['Prefer not to say', 'Female', 'Male', 'Non-binary', 'Other'],\n",
    "        description='Gender:',\n",
    "        value='Prefer not to say'\n",
    "    )\n",
    "    display(gender_dropdown)\n",
    "    \n",
    "    background_dropdown = widgets.Dropdown(\n",
    "        options=['Prefer not to say', 'Computer Science/Engineering', 'Other STEM', \n",
    "                 'Social Sciences', 'Humanities', 'Business', 'Healthcare', 'Other'],\n",
    "        description='Background:',\n",
    "        value='Prefer not to say',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    display(background_dropdown)\n",
    "    \n",
    "    # Final comments\n",
    "    display(HTML(\"<h4>Additional Comments</h4>\"))\n",
    "    final_comments = widgets.Textarea(\n",
    "        placeholder='Any other thoughts about this study or voice-adaptive AI?',\n",
    "        layout=widgets.Layout(width='600px', height='120px')\n",
    "    )\n",
    "    display(final_comments)\n",
    "    \n",
    "    # Submit button\n",
    "    submit_button = widgets.Button(\n",
    "        description='Submit Final Questionnaire',\n",
    "        button_style='primary',\n",
    "        icon='check',\n",
    "        layout=widgets.Layout(width='300px', height='50px')\n",
    "    )\n",
    "    \n",
    "    submit_output = widgets.Output()\n",
    "    \n",
    "    def on_submit(btn):\n",
    "        final_data = {\n",
    "            'participant_id': state.participant_id,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'overall_preference': preference_dropdown.value,\n",
    "            'bias_importance': importance_slider.value,\n",
    "            'future_use_likelihood': likelihood_slider.value,\n",
    "            'concerns': concerns_text.value,\n",
    "            'demographics': {\n",
    "                'age_range': age_dropdown.value,\n",
    "                'gender': gender_dropdown.value,\n",
    "                'background': background_dropdown.value\n",
    "            },\n",
    "            'final_comments': final_comments.value\n",
    "        }\n",
    "        \n",
    "        # Save final data\n",
    "        final_file = config.results_dir / f\"{state.participant_id}_final.json\"\n",
    "        with open(final_file, 'w') as f:\n",
    "            json.dump(final_data, f, indent=2)\n",
    "        \n",
    "        with submit_output:\n",
    "            clear_output()\n",
    "            display(HTML(\"\"\"\n",
    "                <div style='padding: 20px; background-color: #d4edda; border: 1px solid #c3e6cb; border-radius: 5px;'>\n",
    "                    <h3 style='color: #155724;'>‚úì Thank you for completing the study!</h3>\n",
    "                    <p>Your responses have been saved.</p>\n",
    "                    <p><strong>Participant ID:</strong> {}</p>\n",
    "                </div>\n",
    "            \"\"\".format(state.participant_id)))\n",
    "        \n",
    "        btn.disabled = True\n",
    "    \n",
    "    submit_button.on_click(on_submit)\n",
    "    display(submit_button)\n",
    "    display(submit_output)\n",
    "\n",
    "display(final_questionnaire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b63531",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
