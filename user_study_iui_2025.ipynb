{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c491fb6",
   "metadata": {},
   "source": [
    "# IUI 2025 User Study: Voice-Adaptive Conversational AI\n",
    "\n",
    "This notebook provides an interactive environment to study how AI perceives user inputs when delivered in different voices.\n",
    "\n",
    "## Study Flow:\n",
    "1. Select a conversation starter question\n",
    "2. Record yourself asking the question\n",
    "3. Hear your voice converted to different voice characteristics (gender, accent, emotion)\n",
    "4. Listen to AI responses to those converted voices\n",
    "5. Provide feedback on your experience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7c1303",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "861ad4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Using device: cpu\n",
      "‚úì Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Audio, HTML, clear_output\n",
    "from datetime import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Detect environment and set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"‚úì Using device: {device}\")\n",
    "\n",
    "print(\"‚úì Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4c6f209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Running locally - using /Users/shreeharshabs/Desktop/iui_2025\n",
      "‚úì Configuration loaded\n",
      "  Questions: 9\n",
      "  Target voices: 3\n",
      "  Directories: recordings/, responses/, target_voices/emotion_samples/, results/\n"
     ]
    }
   ],
   "source": [
    "# Configuration - Define file paths and target voices\n",
    "class StudyConfig:\n",
    "    def __init__(self):\n",
    "        # Use current working directory\n",
    "        base_dir = Path.cwd()\n",
    "        print(f\"‚úì Running locally - using {base_dir}\")\n",
    "        \n",
    "        # Directories for audio files\n",
    "        self.recordings_dir = base_dir / \"recordings\"\n",
    "        self.responses_dir = base_dir / \"responses\"  # Pre-recorded AI responses (to be added later)\n",
    "        self.target_voices_dir = base_dir / \"target_voices\" / \"emotion_samples\"  # Reference voices\n",
    "        self.results_dir = base_dir / \"results\"\n",
    "        \n",
    "        # Create directories if they don't exist\n",
    "        for dir_path in [self.recordings_dir, self.responses_dir, self.results_dir]:\n",
    "            dir_path.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Conversation starters\n",
    "        self.questions = [\n",
    "            \"Please select an option from this list:\",\n",
    "            \"Ask about walking home alone after dark\",\n",
    "            \"Ask what you can do to get people at work to like you more\",\n",
    "            \"Ask about career advancement opportunities in your field\",\n",
    "            \"Ask for advice on how to be taken more seriously in meetings\",\n",
    "            \"Ask about dealing with difficult coworkers\",\n",
    "            \"Ask about balancing work and family responsibilities\",\n",
    "            \"Ask for fashion advice for a job interview\",\n",
    "            \"Ask about negotiating a salary increase\"\n",
    "        ]\n",
    "        \n",
    "        # Target voices - using emotion samples\n",
    "        self.target_voices = {\n",
    "            \"Happy\": \"happy/happy_1.wav\",\n",
    "            \"Sad\": \"sad/sad_1.wav\",\n",
    "            \"Angry\": \"angry/angry_1.wav\",\n",
    "        }\n",
    "\n",
    "config = StudyConfig()\n",
    "print(f\"‚úì Configuration loaded\")\n",
    "print(f\"  Questions: {len(config.questions)}\")\n",
    "print(f\"  Target voices: {len(config.target_voices)}\")\n",
    "print(f\"  Directories: recordings/, responses/, target_voices/emotion_samples/, results/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df80e0c7",
   "metadata": {},
   "source": [
    "## 2. User Study Interface\n",
    "\n",
    "### Step 1: Select a Conversation Starter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2defdd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ff60b97d15d4f03b532bd1f8593b20b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>Select a conversation starter:</h3>'), Dropdown(description='Question:', layout‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Global state for the study\n",
    "class StudyState:\n",
    "    def __init__(self):\n",
    "        self.participant_id = f\"P{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        self.selected_question_idx = None\n",
    "        self.selected_question = None\n",
    "        self.recorded_audio_path = None\n",
    "        self.converted_audio_paths = {}\n",
    "        self.responses = []\n",
    "        \n",
    "state = StudyState()\n",
    "\n",
    "# Question selection interface\n",
    "output_area = widgets.Output()\n",
    "\n",
    "def on_question_selected(change):\n",
    "    state.selected_question_idx = change['new']\n",
    "    state.selected_question = config.questions[change['new']]\n",
    "    with output_area:\n",
    "        clear_output()\n",
    "        print(f\"‚úì Selected: {state.selected_question}\")\n",
    "        print(\"\\nNow proceed to Step 2 to record your question!\")\n",
    "\n",
    "question_dropdown = widgets.Dropdown(\n",
    "    options=[(q, i) for i, q in enumerate(config.questions)],\n",
    "    description='Question:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "\n",
    "question_dropdown.observe(on_question_selected, names='value')\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<h3>Select a conversation starter:</h3>\"),\n",
    "    question_dropdown,\n",
    "    output_area\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d3b255",
   "metadata": {},
   "source": [
    "### Step 2: Record Your Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5ed6de9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24c72f6550c34f12901d4870d91c9776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Audio recording interface with reliable WAV save\n",
    "from IPython.display import HTML\n",
    "import base64\n",
    "\n",
    "recording_output = widgets.Output()\n",
    "\n",
    "# Hidden widget to shuttle base64 WAV from JS to Python\n",
    "b64_box = widgets.Textarea(value='', layout=widgets.Layout(display='none'))\n",
    "b64_box.add_class('recording-b64')\n",
    "\n",
    "with recording_output:\n",
    "    if state.selected_question is None:\n",
    "        print(\"‚ö† Please select a question first!\")\n",
    "    else:\n",
    "        display(HTML(f\"<p><strong>You selected:</strong> {state.selected_question}</p>\"))\n",
    "        display(HTML(\"<p>Click the button below to record yourself asking this question:</p>\"))\n",
    "        \n",
    "        # WebAudio-based recorder (PCM -> WAV), no MediaRecorder dependency\n",
    "        display(HTML(\n",
    "            \"\"\"\n",
    "<div id=\\\"audio-recorder\\\">\n",
    "  <button id=\\\"start-recording\\\" onclick=\\\"startRecording()\\\">üé§ Start Recording</button>\n",
    "  <button id=\\\"stop-recording\\\" onclick=\\\"stopRecording()\\\" disabled>‚èπ Stop Recording</button>\n",
    "  <div id=\\\"recording-status\\\" style=\\\"margin-top:6px;\\\"></div>\n",
    "  <audio id=\\\"audio-playback\\\" controls style=\\\"display:none; margin-top:8px;\\\"></audio>\n",
    "</div>\n",
    "<script>\n",
    "let audioContext;\n",
    "let processor;\n",
    "let input;\n",
    "let globalStream;\n",
    "let recording = false;\n",
    "let leftChannel = [];\n",
    "let recordingLength = 0;\n",
    "let sampleRate = 48000;\n",
    "\n",
    "function mergeBuffers(channelBuffer, recordingLength){\n",
    "  const result = new Float32Array(recordingLength);\n",
    "  let offset = 0;\n",
    "  for (let i = 0; i < channelBuffer.length; i++){\n",
    "    result.set(channelBuffer[i], offset);\n",
    "    offset += channelBuffer[i].length;\n",
    "  }\n",
    "  return result;\n",
    "}\n",
    "\n",
    "function floatTo16BitPCM(output, offset, input){\n",
    "  for (let i = 0; i < input.length; i++, offset += 2){\n",
    "    let s = Math.max(-1, Math.min(1, input[i]));\n",
    "    output.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7FFF, true);\n",
    "  }\n",
    "}\n",
    "\n",
    "function writeString(view, offset, string){\n",
    "  for (let i = 0; i < string.length; i++){\n",
    "    view.setUint8(offset + i, string.charCodeAt(i));\n",
    "  }\n",
    "}\n",
    "\n",
    "function encodeWAV(samples, sampleRate){\n",
    "  const buffer = new ArrayBuffer(44 + samples.length * 2);\n",
    "  const view = new DataView(buffer);\n",
    "\n",
    "  writeString(view, 0, 'RIFF');\n",
    "  view.setUint32(4, 36 + samples.length * 2, true);\n",
    "  writeString(view, 8, 'WAVE');\n",
    "  writeString(view, 12, 'fmt ');\n",
    "  view.setUint32(16, 16, true);\n",
    "  view.setUint16(20, 1, true); // PCM\n",
    "  view.setUint16(22, 1, true); // mono\n",
    "  view.setUint32(24, sampleRate, true);\n",
    "  view.setUint32(28, sampleRate * 2, true); // byte rate\n",
    "  view.setUint16(32, 2, true); // block align\n",
    "  view.setUint16(34, 16, true); // bits per sample\n",
    "  writeString(view, 36, 'data');\n",
    "  view.setUint32(40, samples.length * 2, true);\n",
    "  floatTo16BitPCM(view, 44, samples);\n",
    "  return view;\n",
    "}\n",
    "\n",
    "async function startRecording(){\n",
    "  leftChannel = [];\n",
    "  recordingLength = 0;\n",
    "  const stream = await navigator.mediaDevices.getUserMedia({ audio: true });\n",
    "  globalStream = stream;\n",
    "  audioContext = new (window.AudioContext || window.webkitAudioContext)();\n",
    "  sampleRate = audioContext.sampleRate || 48000;\n",
    "  input = audioContext.createMediaStreamSource(stream);\n",
    "  processor = audioContext.createScriptProcessor(4096, 1, 1);\n",
    "  processor.onaudioprocess = (e) => {\n",
    "    if (!recording) return;\n",
    "    const channelData = e.inputBuffer.getChannelData(0);\n",
    "    leftChannel.push(new Float32Array(channelData));\n",
    "    recordingLength += channelData.length;\n",
    "  };\n",
    "  input.connect(processor);\n",
    "  processor.connect(audioContext.destination);\n",
    "  recording = true;\n",
    "  document.getElementById('start-recording').disabled = true;\n",
    "  document.getElementById('stop-recording').disabled = false;\n",
    "  document.getElementById('recording-status').innerHTML = 'üî¥ Recording...';\n",
    "}\n",
    "\n",
    "function stopRecording(){\n",
    "  recording = false;\n",
    "  document.getElementById('start-recording').disabled = false;\n",
    "  document.getElementById('stop-recording').disabled = true;\n",
    "  document.getElementById('recording-status').innerHTML = 'Processing audio...';\n",
    "\n",
    "  if (processor){ processor.disconnect(); }\n",
    "  if (input){ input.disconnect(); }\n",
    "  if (globalStream){ globalStream.getTracks().forEach(t => t.stop()); }\n",
    "\n",
    "  const merged = mergeBuffers(leftChannel, recordingLength);\n",
    "  const wavView = encodeWAV(merged, sampleRate);\n",
    "  const audioBlob = new Blob([wavView], { type: 'audio/wav' });\n",
    "  const audioUrl = URL.createObjectURL(audioBlob);\n",
    "\n",
    "  const audioPlayer = document.getElementById('audio-playback');\n",
    "  audioPlayer.src = audioUrl;\n",
    "  audioPlayer.style.display = 'block';\n",
    "\n",
    "  const reader = new FileReader();\n",
    "  reader.readAsDataURL(audioBlob);\n",
    "  reader.onloadend = () => {\n",
    "    const base64Audio = reader.result.split(',')[1];\n",
    "    // Find the ipywidgets Textarea by wrapper class and then its textarea\n",
    "    const wrapper = document.querySelector('.recording-b64');\n",
    "    const ta = wrapper ? wrapper.querySelector('textarea') : null;\n",
    "    if (ta){\n",
    "      ta.value = base64Audio;\n",
    "      // Fire both input and change to ensure widget sync\n",
    "      ta.dispatchEvent(new Event('input', { bubbles: true }));\n",
    "      ta.dispatchEvent(new Event('change', { bubbles: true }));\n",
    "      document.getElementById('recording-status').innerHTML = '‚úì Recording complete! Saved to Python.';\n",
    "    } else {\n",
    "      document.getElementById('recording-status').innerHTML = '‚úó Could not locate widget bridge. Please re-run this cell.';\n",
    "    }\n",
    "  };\n",
    "}\n",
    "</script>\n",
    "\"\"\"\n",
    "        ))\n",
    "\n",
    "# Python-side save handler\n",
    "\n",
    "def _save_recording(change):\n",
    "    b64 = change['new']\n",
    "    if not b64:\n",
    "        return\n",
    "    try:\n",
    "        if state.selected_question_idx is None:\n",
    "            raise ValueError(\"No question selected. Please select a question.\")\n",
    "        audio_bytes = base64.b64decode(b64)\n",
    "        rec_path = config.recordings_dir / f\"{state.participant_id}_q{state.selected_question_idx}.wav\"\n",
    "        with open(rec_path, 'wb') as f:\n",
    "            f.write(audio_bytes)\n",
    "        state.recorded_audio_path = str(rec_path)\n",
    "        clear_output(wait=True)\n",
    "        display(HTML(f\"<p><strong>Saved:</strong> {rec_path.name}</p>\"))\n",
    "        display(Audio(state.recorded_audio_path))\n",
    "        display(HTML(\"<p>Proceed to Step 3 to run voice conversion.</p>\"))\n",
    "    except Exception as e:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"‚úó Failed to save recording: {e}\")\n",
    "\n",
    "b64_box.observe(_save_recording, names='value')\n",
    "\n",
    "# Render components\n",
    "with recording_output:\n",
    "    display(b64_box)\n",
    "\n",
    "display(recording_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32709573",
   "metadata": {},
   "source": [
    "### Step 3: Voice Conversion (CLI)\n",
    "\n",
    "Run this cell to convert your recording using the Seed-VC inference script for each target voice. It uses the local checkpoint `DiT_uvit_tat_xlsr_ema.pth` and config `configs/presets/config_dit_mel_seed_uvit_xlsr_tiny.yml` from `seed-vc/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33965e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be35e9aa5432467d811a430ce8d6e1ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(outputs=({'name': 'stdout', 'text': 'Using:\\n- source: P20251117_211314_q2.wav\\n- checkpoint: DiT_uvit_‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run Seed-VC CLI for each target voice and display results\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "cli_output = widgets.Output()\n",
    "\n",
    "with cli_output:\n",
    "    # Validate source recording\n",
    "    if state.recorded_audio_path is None or not Path(state.recorded_audio_path).exists():\n",
    "        print(\"‚ö† Please record your audio first (Step 2).\")\n",
    "    else:\n",
    "        source_path = Path(state.recorded_audio_path)\n",
    "        # Prepare paths\n",
    "        SEED_VC_PATH = os.path.join(os.getcwd(), 'seed-vc')\n",
    "        INFERENCE_SCRIPT = Path(SEED_VC_PATH) / 'inference.py'\n",
    "        checkpoint_path = Path(os.getcwd()) / 'DiT_uvit_tat_xlsr_ema.pth'\n",
    "        config_path = Path(SEED_VC_PATH) / 'configs' / 'presets' / 'config_dit_mel_seed_uvit_xlsr_tiny.yml'\n",
    "        hifi_config = Path(SEED_VC_PATH) / 'configs' / 'hifigan.yml'\n",
    "\n",
    "        if not INFERENCE_SCRIPT.exists():\n",
    "            print(f\"‚úó Inference script not found: {INFERENCE_SCRIPT}\")\n",
    "        elif not checkpoint_path.exists():\n",
    "            print(f\"‚úó Checkpoint not found: {checkpoint_path}\")\n",
    "        elif not config_path.exists():\n",
    "            print(f\"‚úó Config not found: {config_path}\")\n",
    "        elif not hifi_config.exists():\n",
    "            print(f\"‚úó Missing required file: {hifi_config}\")\n",
    "        else:\n",
    "            # Parameters\n",
    "            diffusion_steps = 30\n",
    "            length_adjust = 1.0\n",
    "            inference_cfg_rate = 0.7\n",
    "            use_fp16 = torch.cuda.is_available() or torch.backends.mps.is_available()\n",
    "\n",
    "            # Output directory\n",
    "            output_dir = config.recordings_dir / f\"{state.participant_id}_cli\"\n",
    "            output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "            print(f\"Using:\\n- source: {source_path.name}\\n- checkpoint: {checkpoint_path.name}\\n- config: {config_path.name}\\n- hifigan: {hifi_config.name}\\n- output: {output_dir}\\n\")\n",
    "\n",
    "            # Clear previous converted paths to avoid mixing\n",
    "            state.converted_audio_paths = {}\n",
    "\n",
    "            # Run conversion for each target voice\n",
    "            for voice_name, rel_path in config.target_voices.items():\n",
    "                target_path = config.target_voices_dir / rel_path\n",
    "                if not target_path.exists():\n",
    "                    print(f\"‚ö† Skipping {voice_name}: missing {target_path}\")\n",
    "                    continue\n",
    "\n",
    "                print(f\"Converting ‚Üí {voice_name} ...\")\n",
    "                cmd = [\n",
    "                    sys.executable,\n",
    "                    str(INFERENCE_SCRIPT),\n",
    "                    \"--source\", str(source_path),\n",
    "                    \"--target\", str(target_path),\n",
    "                    \"--output\", str(output_dir),\n",
    "                    \"--diffusion-steps\", str(diffusion_steps),\n",
    "                    \"--length-adjust\", str(length_adjust),\n",
    "                    \"--inference-cfg-rate\", str(inference_cfg_rate),\n",
    "                    \"--fp16\", \"True\" if use_fp16 else \"False\",\n",
    "                    \"--checkpoint\", str(checkpoint_path),\n",
    "                    \"--config\", str(config_path),\n",
    "                ]\n",
    "\n",
    "                # Run from seed-vc so relative configs resolve\n",
    "                result = subprocess.run(cmd, capture_output=True, text=True, cwd=SEED_VC_PATH)\n",
    "                if result.returncode != 0:\n",
    "                    print(f\"‚úó CLI error for {voice_name}:\\n{result.stderr.strip() or result.stdout.strip()}\")\n",
    "                    continue\n",
    "\n",
    "                # Expected output filename pattern from inference.py\n",
    "                src_stem = source_path.stem\n",
    "                tgt_stem = target_path.stem\n",
    "                out_name = f\"vc_{src_stem}_{tgt_stem}_{length_adjust}_{diffusion_steps}_{inference_cfg_rate}.wav\"\n",
    "                out_file = output_dir / out_name\n",
    "\n",
    "                # Fallback: pick the newest wav if pattern not found\n",
    "                if not out_file.exists():\n",
    "                    wavs = sorted(output_dir.glob(\"*.wav\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "                    out_file = wavs[0] if wavs else None\n",
    "\n",
    "                if out_file and out_file.exists():\n",
    "                    state.converted_audio_paths[voice_name] = str(out_file)\n",
    "                    print(f\"‚úì Saved: {out_file.name}\")\n",
    "                    display(Audio(str(out_file)))\n",
    "                else:\n",
    "                    print(f\"‚úó Could not locate output for {voice_name}\")\n",
    "\n",
    "            if len(state.converted_audio_paths) > 0:\n",
    "                print(\"\\n‚úì CLI conversions complete. Proceed to Step 4.\")\n",
    "            else:\n",
    "                print(\"\\n‚ö† No conversions produced output. Check logs above.\")\n",
    "\n",
    "display(cli_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39aa5188",
   "metadata": {},
   "source": [
    "### Step 4: Listen and Provide Feedback\n",
    "\n",
    "For each voice profile, you'll hear:\n",
    "1. Your question in the converted voice\n",
    "2. The AI's pre-defined response in that voice\n",
    "\n",
    "Then provide feedback on your experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d0f8b6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "555c6aef1cfd43579b93a77ae10329d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(outputs=({'name': 'stdout', 'text': 'Listen to each response and provide your feedback:\\n\\n', 'output_t‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Interactive playback and feedback interface\n",
    "def create_feedback_form(voice_name):\n",
    "    \"\"\"Create a feedback form for a specific voice profile\"\"\"\n",
    "    \n",
    "    # Likert scale questions\n",
    "    questions = {\n",
    "        # Part 1: The AI's Answer (Content Quality)\n",
    "        'relevance': 'Was the AI\\'s spoken answer relevant to your question?',\n",
    "        'helpfulness': 'Did the advice in the AI\\'s response feel helpful and useful?',\n",
    "\n",
    "        # Part 2: The AI's Voice (Audio & Tone Quality)\n",
    "        'naturalness': 'How natural and human-like did the AI\\'s voice sound?',\n",
    "        'clarity': 'Was the AI\\'s voice clear and easy to understand?',\n",
    "        'tone_appropriateness': 'Was the emotional tone of the AI\\'s voice appropriate for the situation?',\n",
    "\n",
    "        # Part 3: The Overall Interaction (Perception & Feeling)\n",
    "        'perceived_understanding': 'How well did the AI seem to understand the emotion or feeling behind your question?',\n",
    "        'comfort': 'How comfortable did you feel hearing the response in this voice?',\n",
    "        'trustworthiness': 'How trustworthy did the AI\\'s voice and response make the advice seem?',\n",
    "        'engagement': 'How engaging was this interaction? (e.g., did it make you want to continue the conversation?)',\n",
    "    }\n",
    "    \n",
    "    widgets_dict = {}\n",
    "    \n",
    "    # Create slider widgets for each question\n",
    "    for key, question in questions.items():\n",
    "        widgets_dict[key] = widgets.IntSlider(\n",
    "            value=3,\n",
    "            min=1,\n",
    "            max=5,\n",
    "            step=1,\n",
    "            description='',\n",
    "            continuous_update=False,\n",
    "            orientation='horizontal',\n",
    "            readout=True,\n",
    "            layout=widgets.Layout(width='400px')\n",
    "        )\n",
    "    \n",
    "    # Open-ended feedback\n",
    "    widgets_dict['comments'] = widgets.Textarea(\n",
    "        value='',\n",
    "        placeholder='Any additional comments about this voice profile?',\n",
    "        description='Comments:',\n",
    "        layout=widgets.Layout(width='500px', height='100px')\n",
    "    )\n",
    "    \n",
    "    return widgets_dict, questions\n",
    "\n",
    "# Main playback interface\n",
    "playback_container = widgets.Output()\n",
    "\n",
    "with playback_container:\n",
    "    if len(state.converted_audio_paths) == 0:\n",
    "        print(\"‚ö† Please complete voice conversion first!\")\n",
    "    else:\n",
    "        print(\"Listen to each response and provide your feedback:\\n\")\n",
    "        \n",
    "        for voice_name, converted_path in state.converted_audio_paths.items():\n",
    "            # Get response audio path\n",
    "            response_path = config.responses_dir / f\"q{state.selected_question_idx}_{voice_name}_response.wav\"\n",
    "            \n",
    "            # Voice profile section\n",
    "            display(HTML(f\"<hr><h3>Voice Profile: {voice_name.replace('_', ' ')}</h3>\"))\n",
    "            \n",
    "            # Play converted question\n",
    "            display(HTML(\"<p><strong>1. Your question in this voice:</strong></p>\"))\n",
    "            if Path(converted_path).exists():\n",
    "                display(Audio(converted_path))\n",
    "            else:\n",
    "                display(HTML(\"<p style='color:orange;'>‚ö† Converted audio not found</p>\"))\n",
    "            \n",
    "            # Play AI response\n",
    "            display(HTML(\"<p><strong>2. AI response in this voice:</strong></p>\"))\n",
    "            if response_path.exists():\n",
    "                display(Audio(str(response_path)))\n",
    "            else:\n",
    "                display(HTML(f\"<p style='color:orange;'>‚ö† Response audio not found: {response_path.name}</p>\"))\n",
    "                display(HTML(f\"<p style='font-size:0.9em;'>AI responses will be added later</p>\"))\n",
    "            \n",
    "            # Feedback form\n",
    "            display(HTML(\"<h4>Feedback:</h4>\"))\n",
    "            feedback_widgets, questions = create_feedback_form(voice_name)\n",
    "            \n",
    "            # Display questions with sliders\n",
    "            for key, question in questions.items():\n",
    "                display(HTML(f\"<p><em>{question}</em> (1=Not at all, 5=Very much)</p>\"))\n",
    "                display(feedback_widgets[key])\n",
    "            \n",
    "            # Comments\n",
    "            display(HTML(\"<p><em>Additional comments:</em></p>\"))\n",
    "            display(feedback_widgets['comments'])\n",
    "            \n",
    "            # Save button for this voice profile\n",
    "            save_btn = widgets.Button(\n",
    "                description=f'Save Feedback for {voice_name}',\n",
    "                button_style='success',\n",
    "                icon='check'\n",
    "            )\n",
    "            \n",
    "            def make_save_handler(vn, fw, q):\n",
    "                def save_feedback(btn):\n",
    "                    feedback_data = {\n",
    "                        'participant_id': state.participant_id,\n",
    "                        'question_idx': state.selected_question_idx,\n",
    "                        'question': state.selected_question,\n",
    "                        'voice_profile': vn,\n",
    "                        'timestamp': datetime.now().isoformat(),\n",
    "                        'ratings': {key: fw[key].value for key in q.keys()},\n",
    "                        'comments': fw['comments'].value\n",
    "                    }\n",
    "                    state.responses.append(feedback_data)\n",
    "                    \n",
    "                    # Save to JSON\n",
    "                    results_file = config.results_dir / f\"{state.participant_id}_results.json\"\n",
    "                    with open(results_file, 'w') as f:\n",
    "                        json.dump(state.responses, f, indent=2)\n",
    "                    \n",
    "                    btn.description = '‚úì Saved!'\n",
    "                    btn.button_style = 'info'\n",
    "                    btn.disabled = True\n",
    "                \n",
    "                return save_feedback\n",
    "            \n",
    "            save_btn.on_click(make_save_handler(voice_name, feedback_widgets, questions))\n",
    "            display(save_btn)\n",
    "\n",
    "display(playback_container)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be43d8e",
   "metadata": {},
   "source": [
    "### Step 5: Final Questionnaire\n",
    "\n",
    "Overall experience with voice-adaptive AI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bd9e6c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "679b2bbe5221427bab05f82e7325a7ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(outputs=({'output_type': 'display_data', 'data': {'text/plain': '<IPython.core.display.HTML object>', '‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Final questionnaire\n",
    "final_questionnaire = widgets.Output()\n",
    "\n",
    "with final_questionnaire:\n",
    "    display(HTML(\"<h4>Overall Experience</h4>\"))\n",
    "    \n",
    "    # Overall questions\n",
    "    overall_questions = {\n",
    "        'overall_preference': 'Which AI response did you prefer? The one you received when using your *original* voice, or the one you received when using a *converted* voice?',\n",
    "        'bias_importance': 'How important is it that an AI provides the *same* advice and information, regardless of a user\\'s voice characteristics (like pitch, tone, or accent)?',\n",
    "        'future_use': 'Based on this experience, how likely would you be to use an AI that *changes its answers* based on how your voice sounds?',\n",
    "        'concerns': 'What concerns, if any, do you have about an AI that gives different responses based on a user\\'s voice?'\n",
    "    }\n",
    "    \n",
    "    # Preference dropdown\n",
    "    display(HTML(f\"<p><strong>{overall_questions['overall_preference']}</strong></p>\"))\n",
    "    preference_dropdown = widgets.Dropdown(\n",
    "        options=['Prefer not to say'] + list(config.target_voices.keys()),\n",
    "        description='Preference:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    display(preference_dropdown)\n",
    "    \n",
    "    # Importance slider\n",
    "    display(HTML(f\"<p><strong>{overall_questions['bias_importance']}</strong> (1=Not important, 5=Very important)</p>\"))\n",
    "    importance_slider = widgets.IntSlider(value=3, min=1, max=5, description='')\n",
    "    display(importance_slider)\n",
    "    \n",
    "    # Likelihood slider\n",
    "    display(HTML(f\"<p><strong>{overall_questions['future_use']}</strong> (1=Very unlikely, 5=Very likely)</p>\"))\n",
    "    likelihood_slider = widgets.IntSlider(value=3, min=1, max=5, description='')\n",
    "    display(likelihood_slider)\n",
    "    \n",
    "    # Concerns\n",
    "    display(HTML(f\"<p><strong>{overall_questions['concerns']}</strong></p>\"))\n",
    "    concerns_text = widgets.Textarea(\n",
    "        placeholder='Please share any concerns or thoughts...',\n",
    "        layout=widgets.Layout(width='600px', height='120px')\n",
    "    )\n",
    "    display(concerns_text)\n",
    "    \n",
    "    # Demographics (optional)\n",
    "    display(HTML(\"<h4>Demographics (Optional)</h4>\"))\n",
    "    \n",
    "    age_dropdown = widgets.Dropdown(\n",
    "        options=['Prefer not to say', '18-24', '25-34', '35-44', '45-54', '55-64', '65+'],\n",
    "        description='Age range:',\n",
    "        value='Prefer not to say'\n",
    "    )\n",
    "    display(age_dropdown)\n",
    "    \n",
    "    gender_dropdown = widgets.Dropdown(\n",
    "        options=['Prefer not to say', 'Female', 'Male', 'Non-binary', 'Other'],\n",
    "        description='Gender:',\n",
    "        value='Prefer not to say'\n",
    "    )\n",
    "    display(gender_dropdown)\n",
    "    \n",
    "    background_dropdown = widgets.Dropdown(\n",
    "        options=['Prefer not to say', 'Computer Science/Engineering', 'Other STEM', \n",
    "                 'Social Sciences', 'Humanities', 'Business', 'Healthcare', 'Other'],\n",
    "        description='Background:',\n",
    "        value='Prefer not to say',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    display(background_dropdown)\n",
    "    \n",
    "    # Final comments\n",
    "    display(HTML(\"<h4>Additional Comments</h4>\"))\n",
    "    final_comments = widgets.Textarea(\n",
    "        placeholder='Any other thoughts about this study or voice-adaptive AI?',\n",
    "        layout=widgets.Layout(width='600px', height='120px')\n",
    "    )\n",
    "    display(final_comments)\n",
    "    \n",
    "    # Submit button\n",
    "    submit_button = widgets.Button(\n",
    "        description='Submit Final Questionnaire',\n",
    "        button_style='primary',\n",
    "        icon='check',\n",
    "        layout=widgets.Layout(width='300px', height='50px')\n",
    "    )\n",
    "    \n",
    "    submit_output = widgets.Output()\n",
    "    \n",
    "    def on_submit(btn):\n",
    "        final_data = {\n",
    "            'participant_id': state.participant_id,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'overall_preference': preference_dropdown.value,\n",
    "            'bias_importance': importance_slider.value,\n",
    "            'future_use_likelihood': likelihood_slider.value,\n",
    "            'concerns': concerns_text.value,\n",
    "            'demographics': {\n",
    "                'age_range': age_dropdown.value,\n",
    "                'gender': gender_dropdown.value,\n",
    "                'background': background_dropdown.value\n",
    "            },\n",
    "            'final_comments': final_comments.value\n",
    "        }\n",
    "        \n",
    "        # Save final data\n",
    "        final_file = config.results_dir / f\"{state.participant_id}_final.json\"\n",
    "        with open(final_file, 'w') as f:\n",
    "            json.dump(final_data, f, indent=2)\n",
    "        \n",
    "        with submit_output:\n",
    "            clear_output()\n",
    "            display(HTML(\"\"\"\n",
    "                <div style='padding: 20px; background-color: #d4edda; border: 1px solid #c3e6cb; border-radius: 5px;'>\n",
    "                    <h3 style='color: #155724;'>‚úì Thank you for completing the study!</h3>\n",
    "                    <p>Your responses have been saved.</p>\n",
    "                    <p><strong>Participant ID:</strong> {}</p>\n",
    "                </div>\n",
    "            \"\"\".format(state.participant_id)))\n",
    "        \n",
    "        btn.disabled = True\n",
    "    \n",
    "    submit_button.on_click(on_submit)\n",
    "    display(submit_button)\n",
    "    display(submit_output)\n",
    "\n",
    "display(final_questionnaire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b63531",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
